fix for prod timeout exception, Can be implemented in other Flink Jobs

Step1:Updated pom with bellow versions,
<flink.version>1.14.3</flink.version>
<scala.binary.version>2.12</scala.binary.version>\

Step2:Added values for uid/consumer group name		
--uid_suffix UidQa			//in yml config file
private static final String UID_SUFFIX = "uid_suffix";

--uid_suffix UidDev --consumer_group_name flink_mar_dev
private static final String CONSUMER_GROUP_NAME = "consumer_group_name";
String consumerGroupName = StringUtils.isNullOrWhitespaceOnly(params.get(CONSUMER_GROUP_NAME))
				? "flink_mar_code"
				: params.get(CONSUMER_GROUP_NAME);
Step3:Removed old classes with Source/Sink
KafkaSource<Tuple3<String, String, String>> source = KafkaSource.<Tuple3<String, String, String>>builder()
				.setBootstrapServers(bootstrapServers).setTopics(sourceTopics)
				.setDeserializer(new MarOrderWrapperSchema()).setGroupId("na")
				.setStartingOffsets(OffsetsInitializer.earliest()).build();
		DataStream<Tuple3<String, String, String>> sourceData = env.fromSource(source,
				WatermarkStrategy.<Tuple3<String, String, String>>noWatermarks(), "enrichedTradeSource")
				.uid("enrichedTradeSource" + uidSuffix);
KafkaSink<Tuple2<String, String>> marNordpoolIntradayProducer = KafkaSink.<Tuple2<String, String>>builder()
				.setBootstrapServers(bootstrapServers)
				.setKafkaProducerConfig(
						TransformationConfig.getKafkaProducerProperties(bootstrapServers, useSSL, keytabLocation,
								keytabPrincipal, truststorePath, truststorePassword, keystorePath, keystorePassword))
				.setDeliverGuarantee(DeliveryGuarantee.EXACTLY_ONCE).setTransactionalIdPrefix("my-trx-id-prefix")
				.setRecordSerializer(KafkaRecordSerializationSchema.builder().setTopic(outputTopic)
						.setValueSerializationSchema(new MarProducerSerializationSchema()).build())
				.build();
		nordpoolTransformedJsonData.sinkTo(marNordpoolIntradayProducer).name("enrichedTradeSource")
				.uid("enrichedTradeSource" + uidSuffix);
				
Step4:Updated Serilized classes with required implementation,
1.2.
updated implements KafkaDeserializationSchema<Tuple3<String,String,String>> with implements KafkaRecordDeserializationSchema<Tuple3<String,String,String>>

updated ProducerSerializationSchema implements SerializationSchema<Tuple2<String,String>>

Step5:
		//commented bellow
		/*
		 * env.getCheckpointConfig().setMaxConcurrentCheckpoints(
		 * maxConcurrentCheckpoint == null ?
		 * JobConfigConstants.MAX_CONCURRENT_CHECKPOINTING :
		 * Integer.parseInt(maxConcurrentCheckpoint));
		 */